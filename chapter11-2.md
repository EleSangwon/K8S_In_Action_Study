# Chapter 11 - 쿠버네티스 내부

## 컨트롤러의 상호 협력 방식
```
쿠버네티스의 동작 원리에 대한 이해를 위해 포드의 리소스가 생성될 때
무슨 일이 발생하는 지 알아본다.
```

## 관련된 컴포넌트의 이해
```
전체 프로세스를 시작하기 전에도 컨트롤러, 스케줄러, Kubelet 은 각 리소스 유형의
변경 사항을 API 서버에서 감시하고 있다. 
그림에 etcd가 없는 이유는 API 서버 뒤에 숨겨져 있기 때문이다.
그래서 API 서버를 객체가 저장되는 공간으로 생각할 수 있다.
```
![image](https://user-images.githubusercontent.com/50174803/136661740-1db8286b-204c-4a2f-8f9b-5f6bb218934a.png)

## 이벤트 체인
```
배포 매니패스트가 포함하는 YAML 파일을 준비하고 이를 kubectl 을 통해서 쿠버네티스에 
제출한다고 상상해보자.

kubectl 은 쿠버네티스 API 서버로 HTTP POST 요청으로 매니패스트를 보낸다.
API 서버는 배포 명세를 확인하고 etcd에 저장한다.
그리고 kubectl 에 응답한다.
```
![image](https://user-images.githubusercontent.com/50174803/136661876-01f63ca6-3ab2-4c98-a919-4f93f110f82b.png)

## 포드의 컨테이너를 실행하는 Kubelet 
```
특정 노드에 스케줄되는 포드와 함께 노드의 Kubelet은 일을 시작한다.
API 서버에서 포드의 변화를 감시하는 Kubelet은 노드로 스케줄되는 새로운 포드를 살펴본다.
그래서 포드의 정의를 조사하고 포드의 컨테이너를 시작하기 위해 도커나 컨테이너 런타임 등에게 알려준다.
그런 후에 컨테이너 런타임은 컨테이너를 실행한다.
```

## 실행 중인 포드의 이해
```
kubectl run nginx --image=nginx 명령어를 통해 단일 컨테이너 포드를 실행한다.
그 이후 docker ps 를 통해 모든 컨테이너를 나열할 수 있다.
nginx 컨테이너 뿐 아니라 추가 컨테이너를 볼 수 있다. 
일시 정지된 컨테이너는 포드의 모든 컨테이너를 함께 담고 있는 컨테이너다.

일시 정지된 컨테이너는 인프라스트럭처 컨테이너로써 모든 네임스페이스를 보유하는 것이
고유한 목적이다. 포드의 모든 사용자 정의 컨테이너는 포드 인프라스트럭처 컨테이너의
네임스페이스를 사용한다.
```
![image](https://user-images.githubusercontent.com/50174803/136662534-d5e1a6a6-03ca-4c66-90a8-71233a381ceb.png)
![image](https://user-images.githubusercontent.com/50174803/136662661-c90770a4-7016-430b-9d79-81f4e60d00d2.png)

## 인터-포드 네트워킹 
```
네트워크는 시스템 관리자나 컨테이너 네트워크 플러그인에 의해 설정된다.
쿠버네티스가 자체적으로 수행하는 것이 아니다.

쿠버네티스는 특정 네트워크 기술을 사용하라고 요구하지 않지만, 포드는 동일한 워커노드에 위치하는 지
여부와 관계없이 다른포드들과 커뮤니케이션할 수 있어야 한다.

포드가 통신하는 데 사용하는 네트워크는 포드가 자신의 것으로 보는 IP주소가 문제되는 포드의 IP주소와
정확히 동일한 네트워크여야 한다.

아래 그림을 보면 포드 A는 네트워크 패킷을 보내기 위해 포드 B에 연결하려 할 때 포드 B가 보는 소스 IP는
포드A가 보는 동일한 IP이어야 한다. 여기에 그들 사이에 NAT는 없다. 

포드 A가 보낸 패킷은 반드시 소스와 도착지 주소의 변경 없이 포드 B에 도달해야 한다.
```
![image](https://user-images.githubusercontent.com/50174803/137086670-7309539d-086a-4f89-876a-084f78b6863f.png)

## 네트워크 동작 원리 깊이 알아보기
```
앞에서 포드의 IP주소와 네트워크 네임스페이스를 셋업하고 인프라스트럭쳐 컨테이너(pause 컨테이너)가 점유하는 것을 보았다.
그리고 나서 포드의 컨테이너는 인프라스트럭쳐 컨테이너 네트워크 네임스페이스를 사용한다.
따라서 포드의 네트워크 인터페이스는 인프라스트럭쳐 컨테이너 내에서 설정된 것이다.

* 동일한 노드상의 포드 간의 통신 활성화

인프라스트럭쳐 컨테이너가 시작되기 전에 가상 이더넷 인터페이스 쌍(veth 쌍)은 컨테이너를 위해 생성된다.
인터페이스 쌍 중 하나의 인터페이스는 호스트의 네임스페이스에 존재한다.
반면에 다른 인터페이스쌍은 컨테이너의 네트워크 네임스페이스로 옮겨지고 veth0로 이름도 변경된다.

호스트의 네트워크 네임스페이스의 인터페이스는 컨테이너 런타임에서 사용하도록 구성된 네트워크 브릿지에 연결된다.
컨테이너의 eth0 인터페이스는 브릿지의 주소 범위 내에서 IP주소를 할당한다.

컨테이너 내부에서 실행되고 있는 애플리케이션은 eth0 네트워크 인터페이스로 보내는 모든 것이 호스트 네임스페이스의
다른 veth 인터페이스에서 나오고 브릿지로 전송한다.

이것은 브릿지로 연결된 모든 네트워크 인터페이스에서 수신할 수 있음을 의미한다.

포드A가 포드B로 네트워크 패킷을 보내면, 그 패킷은 포드A의 쌍을 통해 브릿지로 이동한 다음 포드B의 쌍을 통해 이동한다.
노드의 모든 컨테이너는 동일한 브릿지에 연결되므로 서로 통신할 수 있다.

그러나 다른 노드에서 실행되는 컨테이너와 통신할 수 있게 하려면 해당 노드의 브릿지를 어떻게든 연결해야 한다.

```
![image](https://user-images.githubusercontent.com/50174803/137087281-968e03e4-2cd3-4d4c-b5e4-a0bb4e37b6a8.png)

```
* 서로 다른 노드상의 포드와의 통신 활성화

다른 노드에서 브릿지를 연결하는 방법은 여러 가지가 있다. 오버레이 네트워크 또는 layer3 라우팅을 통해 이 작업을
수행할 수 있다.

포드 IP주소가 전체 클러스터에서 유일해야 한다는 것을 알고 있으므로 여러 노드의 포드가 동일한 IP를 획득하는 것을
예방하려면 노드의 브릿지가 겹치지 않는 주소 범위를 사용해야 한다.

아래 그림에서 노드 A의 브릿지는 10.1.1.0/24 IP 범위를 사용하고 노드 B의 브릿지는 10.1.2.0/24 를 사용하므로
IP 주소 충돌이 발생하지 않는다.

아래 그림은 일반 계층 3 네트워킹을 사용하는 두 노드에서 포드 간의 통신을 가능하게 하기 위해 노드의 물리적 네트워크
인터페이스도 브릿지에 연결해야 함을 보여준다.

10.1.2.0/24로 향하는 모든 패킷이 노드 B로 라우팅되도록 노드 A의 라우팅 테이블을 구성해야 하고 
10.1.1.0/24 로 보낸 패킷을 노드 A로 라우팅되도록 노드 B의 라우팅 테이블을 구성해야 한다.

이 유형의 설정을 사용하면 노드 중 하나의 컨테이너에서 다른 노드의 컨테이너로 패킷을 보낼 때,
패킷은 먼저 veth 페어를 통과한 다음, 브릿지를 통해 노드의 실제 어댑터로 전달된다.
그런 후 케이블을 통해 다른 노드의 물리적 어댑터에 도달하고 마지막으로 
다른 노드의 브릿지인 대상 컨테이너의 veth 페어를 통과한다.

이 기능은 노드가 동일한 네트워크 스위치에 연결돼 있고 중간에 라우터가 없는 경우에만 동작한다.
그렇지 않으면 라우터가 개인 IP 주소인 포드 IP를 참조하기 때문에 패킷을 드롭한다.

* 컨테이너 네트워크 인터페이스 CNI 소개

컨테이너를 네트워크에 쉽게 연결하기 위해 CNI라는 프로젝트가 시작됐다.
CNI는 쿠버네티스가 외부에 있는 CNI 플러그인을 사용하도록 구성할 수 있다.

```
![image](https://user-images.githubusercontent.com/50174803/137088727-e53716fc-bec6-4bb1-9ddc-4c474fcb1f10.png)

## 서비스의 구현 방식 
```

* kube-proxy 소개

서비스와 관련된 모든 것은 각 노드에서 실행되는 kube-proxy 프로세스에 의해 처리된다.
초기에는 실제 프록시였지만(userspace 프록시모드) 나중에는 성능이 더 우수한 모드로 교체됐다.
(iptables 프록시 모드)

각 서비스가 고유의 안정된 IP주소와 포트를 얻는다는 것을 안다.

클라이언트는 IP주소와 포트를 연결해 서비스를 사용한다.
IP주소는 가상이다. 서비스의 주요 세부사항은 IP 및 포트쌍으로 구성되므로
서비스 IP 자체는 아무것도 나타내지 않는다. 그래서 핑을 할 수 없다.

--> 서비스 IP는 가상이므로 Ping을 할 수 없다.

* kube-proxy가 iptables를 사용하는 방식

API 서버에서 서비스를 만들면 가상IP주소가 즉시 할당된다.
그 후 API 서버는 워커노드에서 실행중인 모든 kube-proxy 에이전트에
새로운 서비스가 생성됨을 알린다.

그런 다음 각 kube-proxy는 노드에서 실행 중인 해당 서비스에 주소를
지정할 수 있게 한다.

* kube-proxy 를 사용해 클라이언트가 서비스를 통해 포드에 연결하는 방법

클라이언트 포드(포드A)가 패킷을 보낼 때 패킷에 어떤 일이 일어나는 지
살펴보자.
패킷의 목적지는 처음에 서비스의 IP와 포트로 설정된다. (172.30.0.1:80)
네트워크로 보내지기 전에 패킷은 먼저 노드에 설정된 iptables 규칙에 따라
노드 A의 커널에 의해 처음 처리된다.

커널은 패킷이 iptables 규칙 중 하나와 일치하는 지 확인한다.
그들 중 하나는 어떤 패킷이 목적지 IP가 172.30.0.1 이고 목적지 포트가 80인 경우,
패킷의 목적지 IP와 포트는 무작위로 선택된 포드의 IP와 포트로 대체돼야 한다고
명령한다.

그림의 패킷은 해당 규칙과 일치하므로 대상 IP/포트가 변경된다.
그림에서 포드B2는 랜덤으로 선택됐고, 패킷의 대상 IP는 10.1.2.1(포드B2의 IP)로 
변경되고 포트는 8080으로 변경된다. 
```
![image](https://user-images.githubusercontent.com/50174803/138419328-a4efa4a8-70f8-4741-bea9-2fae3e972780.png)

## 고가용성 클러스터 실행
```
쿠버네티스 내부에서 애플리케이션을 실행하는 이유 중 하나는 인프라스트럭쳐 장애 시에도, 인터럽트나 제한적인 사용자 개입 없이 지속적으로 애플리케이션을 실행하기 위해서다. 중단 없이 서비스를 실행하려면 애플리케이션만이 항상 작동해야 하는 것이 아니라 쿠버네티스 컨트롤 플레인 컴포넌트도 항상 작동해야 한다.

* 앱의 가용성 높이기

쿠버네티스에서 앱을 실행할 때, 노드가 실패한 경우에도 다양한 컨트롤러가 앱이 지정된 스케일에서 원활하게 실행되도록 한다. 

애플리케이션의 가용성을 높이려면 디플로이먼트 리소스를 통해 애플리케이션을 실행하고 적절한 수의 복제본을 구성하기만 하면 된다.

* 쿠버네티스 컨트롤 플레인 컴포넌트의 가용성 높이기

쿠버네티스의 가용성을 높이려면 다음 컴포넌트의 여러 인스턴스를 실행하는 여러 마스터 노드를 실행해야 한다.

1) 모든 API 객체가 보관되는 분산 데이터 스토리지인 etcd
2) API 서버
3) 모든 컨트롤러가 실행되는 프로세스인 컨트롤러 매니저
4) 스케줄러 

```
![image](https://user-images.githubusercontent.com/50174803/138423996-16a1fb1b-aa2e-4d82-9297-cab118814d8e.png)
